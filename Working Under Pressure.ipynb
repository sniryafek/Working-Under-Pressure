{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Under Pressure: Predicting NBA Players Performance in the Playoffs (championship games) Using Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Michael Jordan is considered to be the best player ever played basketball. One of the main reasons is that he elevated his performance when it mattered the most - In the NBA Playoffs (the NBA championship games). For example, while in the regular season he scored 30.1 points on 50.1% eFG% (efficient Field Goal Percentge), in the playoff , where the competition intensifies, he improved his averages and scored 33.4 points per game on 50.3% eFG%.\n",
    "\n",
    "In this project, I tried to predict which NBA players preform better in the playoffs, based on their eFG% stats.\n",
    "The eFG% measure considered to be a better measure for shooting ability as it gives higher weight to the more valuable 3 point shooting compared with 2 point shooting. \n",
    "\n",
    "In the project, I use a random forest binary classifier. the response variable is assigned a value of 1 if a player improved his efG% in the playoffs compared to his eFG% in the regular season and 0 otherwise. My independent variables are the player's personal stats in the regular season and some player's team stats measures.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create the data set, I scraped the players' personal stats data from \"Basketball Reference\" website using the BeautifulSoup Python's library and store the data into a data frame.\n",
    "The data includes regular per game stats (points, assists, etc.) and more advanced stats (inter alia, efficiency measures like PER, True Shooting percentage, Win shares etc.) \n",
    "Similarly, I scraped the player's team stats data from the \"NBA Miner\" Website.\n",
    "The data is ranged between 1997-2019 where data for earlier years in not available on NBA Miner Website.\n",
    "\n",
    "***Important*** - \n",
    "There is no need to run the scraping code below, which could take a long time, as the files needed for the rest of the project have already been stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the code below is ot needed. Files are already saved.\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#scraping players' stats data from basketball reference website\n",
    "years = np.arange(1997,2020)\n",
    "url_dic = {\n",
    "            'pg': \"https://www.basketball-reference.com/leagues/NBA_{}_per_game.html\",\n",
    "            'advanced': \"https://www.basketball-reference.com/leagues/NBA_{}_advanced.html\",\n",
    "            'playoffs_pg': \"https://www.basketball-reference.com/playoffs/NBA_{}_per_game.html\"\n",
    "          }\n",
    "for url in url_dic:\n",
    "    for year in years:\n",
    "        html = urlopen(url_dic[url].format(year))\n",
    "        soup = BeautifulSoup(html)\n",
    "        #get column headers\n",
    "        headers = [th.getText() for th in soup.findAll('tr', limit=2)[0].findAll('th')]\n",
    "        headers = headers[1:]\n",
    "        #get stats data by player \n",
    "        rows = soup.findAll('tr')[1:]\n",
    "        player_stats = [[td.getText() for td in rows[i].findAll('td')]\n",
    "                    for i in range(len(rows))]\n",
    "        stats = pd.DataFrame(player_stats, columns = headers)\n",
    "        #change numeric data to numeric format\n",
    "        stats['Age'] = pd.to_numeric(stats['Age'])\n",
    "        stats_numeric = stats.iloc[:,4:].apply(pd.to_numeric, errors='coerce')\n",
    "        stats.iloc[:,4:] = stats_numeric\n",
    "        #add year column\n",
    "        listyear = [year]*len(stats.index)\n",
    "        stats.insert(loc=0, column ='Year', value = listyear)\n",
    "        #concat year data to main tbale collecting all years stats\n",
    "        if year == years[0]:\n",
    "            stats_data = stats\n",
    "        else:\n",
    "            stats_data=pd.concat([stats_data, stats]).reset_index(drop=True)\n",
    "    \n",
    "    stats_data.to_csv('stats_data_' + url + '.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the code below is not needed. Files are already saved.\n",
    "\n",
    "#scraping players' team stats data from nba miner website\n",
    "url = \"http://www.nbaminer.com/nbaminer_nbaminer/advanced_team_stats.php?partitionpage={}\"\n",
    "years = np.arange(1,24)\n",
    "for year in years:\n",
    "    html = urlopen(url.format(year))\n",
    "    soup = BeautifulSoup(html)\n",
    "    table = soup.find('table', id=\"team_statsGrid\")\n",
    "    rows = table.findAll('tr')\n",
    "    headers = [th.getText().strip() for th in rows[1].findAll('th')]\n",
    "    #handling the fact that until 24 there were 29 teams in the league and 30 teams later on\n",
    "    if year<16:\n",
    "        rows = rows[3:33]\n",
    "    else:\n",
    "        rows = rows[3:32]\n",
    "        \n",
    "    player_stats = [[td.getText().strip() for td in rows[i].findAll('td')] \n",
    "                   for i in range(len(rows))]\n",
    "    stats = pd.DataFrame(player_stats, columns = headers)\n",
    "    #get image caption for team name\n",
    "    a = [img[\"title\"] for img in table.select(\"img[title]\")]\n",
    "    stats[\"Team\"] = a \n",
    "    stats.iloc[:,3:] = stats.iloc[:,3:].apply(pd.to_numeric, errors='coerce')\n",
    "    stats = stats.iloc[:,1:]\n",
    "    listyear = [2020-year]*len(stats.index)\n",
    "    stats.insert(loc=0, column ='Year', value = listyear)\n",
    "    if year == years[0]:\n",
    "        stats_data = stats\n",
    "    else:\n",
    "        stats_data=pd.concat([stats_data, stats]).reset_index(drop=True)\n",
    "    \n",
    "stats_data.to_csv('stats_team_advanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let import the required python libraries. Then, I import and concatenate into one dataset the two \"regular season\" stats datasets (Per Game stats and Advanced stats). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "#import and concat regular seson per game and advanced stats - \n",
    "data_pg = pd.read_csv('stats_data_pg.csv')\n",
    "data_advanced = pd.read_csv('stats_data_advanced.csv')\n",
    "data_season = pd.concat([data_pg.iloc[:, 1:], data_advanced.iloc[:, 8:]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let see how many observations there is in the data set for each player:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nazr Mohammed    28\n",
       "Andre Miller     25\n",
       "Vince Carter     25\n",
       "Joe Smith        25\n",
       "Joe Johnson      23\n",
       "Name: Player, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_season['Player'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a first look, it seems unreasonable that the data spans 23 years (1997-2019) but there are players with larger number of observations in the dataset. But we need to remember that during the regular season, players can move between teams. For Example, let take a look Nazr Mohammed's (the top frequent player in the dataset) teams over the years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Tm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>1999</td>\n",
       "      <td>PHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>2000</td>\n",
       "      <td>PHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2554</th>\n",
       "      <td>2001</td>\n",
       "      <td>TOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2555</th>\n",
       "      <td>2001</td>\n",
       "      <td>PHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556</th>\n",
       "      <td>2001</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>2002</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>2003</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4160</th>\n",
       "      <td>2004</td>\n",
       "      <td>TOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4161</th>\n",
       "      <td>2004</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4162</th>\n",
       "      <td>2004</td>\n",
       "      <td>NYK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4767</th>\n",
       "      <td>2005</td>\n",
       "      <td>TOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4768</th>\n",
       "      <td>2005</td>\n",
       "      <td>NYK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4769</th>\n",
       "      <td>2005</td>\n",
       "      <td>SAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5361</th>\n",
       "      <td>2006</td>\n",
       "      <td>SAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5931</th>\n",
       "      <td>2007</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6516</th>\n",
       "      <td>2008</td>\n",
       "      <td>TOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517</th>\n",
       "      <td>2008</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6518</th>\n",
       "      <td>2008</td>\n",
       "      <td>CHA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7129</th>\n",
       "      <td>2009</td>\n",
       "      <td>CHA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7753</th>\n",
       "      <td>2010</td>\n",
       "      <td>CHA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8369</th>\n",
       "      <td>2011</td>\n",
       "      <td>TOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8370</th>\n",
       "      <td>2011</td>\n",
       "      <td>CHA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8371</th>\n",
       "      <td>2011</td>\n",
       "      <td>OKC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8972</th>\n",
       "      <td>2012</td>\n",
       "      <td>OKC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9553</th>\n",
       "      <td>2013</td>\n",
       "      <td>CHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10178</th>\n",
       "      <td>2014</td>\n",
       "      <td>CHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10821</th>\n",
       "      <td>2015</td>\n",
       "      <td>CHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11471</th>\n",
       "      <td>2016</td>\n",
       "      <td>OKC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Year   Tm\n",
       "1487   1999  PHI\n",
       "2024   2000  PHI\n",
       "2554   2001  TOT\n",
       "2555   2001  PHI\n",
       "2556   2001  ATL\n",
       "3095   2002  ATL\n",
       "3598   2003  ATL\n",
       "4160   2004  TOT\n",
       "4161   2004  ATL\n",
       "4162   2004  NYK\n",
       "4767   2005  TOT\n",
       "4768   2005  NYK\n",
       "4769   2005  SAS\n",
       "5361   2006  SAS\n",
       "5931   2007  DET\n",
       "6516   2008  TOT\n",
       "6517   2008  DET\n",
       "6518   2008  CHA\n",
       "7129   2009  CHA\n",
       "7753   2010  CHA\n",
       "8369   2011  TOT\n",
       "8370   2011  CHA\n",
       "8371   2011  OKC\n",
       "8972   2012  OKC\n",
       "9553   2013  CHI\n",
       "10178  2014  CHI\n",
       "10821  2015  CHI\n",
       "11471  2016  OKC"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_season.loc[data_season['Player']=='Nazr Mohammed',['Year', 'Tm']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in several occasions Mohammed played for more than one team and in those cases the dataset also includes the TOT ('Total') observation for aggregating the stats over the different teams he played in one season. The dataset is already sorted in a way that the last team for every player and year in the dataset is also the last team the player actually played for in that year (which is also the team that the player played for in the playoffs). Since my goal is to compare the player performance in regular season to the playoffs, I keep only the data of the last team the player played for. Preforming that, we can see that the player value counts make more sense (and more familiar names are most frequent in the dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dirk Nowitzki    21\n",
       "Vince Carter     21\n",
       "Kevin Garnett    20\n",
       "Kobe Bryant      20\n",
       "Jason Terry      19\n",
       "Name: Player, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop duplicates and keep only the players' last team stats. \n",
    "data_season.drop_duplicates(subset =['Year', 'Player'],  keep = 'last', inplace = True)\n",
    "data_season = data_season.reset_index(drop=True)\n",
    "data_season['Player'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with Nan Values (both for regular season and playoff datasets):\n",
    "\n",
    "- Drop garbage columns\n",
    "\n",
    "- Drop observations of players which didn't shot the ball at all (FGA>0).\n",
    "\n",
    "- fill 3P% column's Nan values with 0 as almost all of these observations are front line players which are not capable of shooting 3s.\n",
    "\n",
    "- Fill 2p% and FT% column with the mean value of the column.\n",
    "\n",
    "In this piece of code, I also add new feature - Total FGA - which I use when merging the datasets.\n",
    "then, I merge the regular season and playoffs datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_season = data_season.drop(['\\xa0', '\\xa0.1'], axis=1)\n",
    "data_season = data_season[data_season['FGA']>0]\n",
    "data_season['3P%'].fillna(0, inplace=True)\n",
    "data_season['2P%'].fillna(data_season['2P%'].mean(), inplace=True)\n",
    "data_season['FT%'].fillna(data_season['FT%'].mean(), inplace=True)\n",
    "data_season['Total_FGA'] = data_season['FGA']*data_season['G']  \n",
    "\n",
    "#import playoffs data\n",
    "data_playoffs = pd.read_csv('stats_data_playoffs_pg.csv')\n",
    "#data preprocesing handle na values\n",
    "data_playoffs = data_playoffs.dropna(subset=['Player'])\n",
    "data_playoffs = data_playoffs[data_playoffs['FGA']>0]\n",
    "data_playoffs['3P%'].fillna(0, inplace=True)\n",
    "data_playoffs['2P%'].fillna(data_playoffs['2P%'], inplace=True)\n",
    "data_playoffs['FT%'].fillna(data_playoffs['FT%'].mean(), inplace=True)\n",
    "data_playoffs['Total_FGA'] = data_playoffs['FGA']*data_playoffs['G']\n",
    "\n",
    "#merge season and playoffs data\n",
    "data = pd.merge(data_season, data_playoffs[['Year', 'Player', 'eFG%', 'Total_FGA']], on=['Year', 'Player'], how='left', \n",
    "                suffixes = ('', '_Pfs'), indicator = 'Exist')\n",
    "data = data[data['Exist'] == 'both'].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I enrich the data with players' team stats. here I add some features (Wins, Pace) of players own team. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import teams' stats data\n",
    "\n",
    "#handling different names for same teams\n",
    "pd.options.mode.chained_assignment = None  # ignoring copy warning\n",
    "data.Tm[data['Tm']=='CHH'] = 'CHO'\n",
    "data.Tm[data['Tm']=='WSB'] = 'WAS'\n",
    "\n",
    "team = pd.read_csv('stats_team_advanced.csv').iloc[:,1:]\n",
    "Team_key = pd.read_csv('Team_key.csv')\n",
    "team = pd.merge(team, Team_key, on=['Team'], how='left')\n",
    "data = pd.merge(data,team[['Year', 'Tm', 'Win', 'Pace']], on=['Year', 'Tm'], how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I add a feature that relates to the quality of defense the Player is facing. Against good defensive teams it would be much harder to improve the player's personal stats compared with poor defensive teams. Hence, I calculate the weighted average of the efg% that the players' opponents in the playoffs allow to their rivals in the regular season. The efG% is weighted by the number of games that the player played against each team in the playoffs.\n",
    "\n",
    "In the Block of code Below:\n",
    "\n",
    "- I import and preprocess the file 'Seriss.csv' downloaded from 'Basketball Reference' which contains all the playoffs matchups in NBA History.\n",
    "\n",
    "- I merge the opp. EFG% feature, taken from the team's stats dataset, with the Series dataset  .\n",
    "\n",
    "- I calculate the weighted average of the quality of defense measure and merge that feature to the main dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and preprocess Series dataset\n",
    "Series = pd.read_csv('Series.csv')[['Yr', 'Team', 'W', 'Team.1', 'W.1']]\n",
    "Series = Series[Series.Yr>=1997]\n",
    "Series2 = Series[['Yr', 'Team.1', 'W.1', 'Team', 'W']]\n",
    "Series2.columns = Series.columns\n",
    "Series = pd.concat([Series, Series2]).sort_values(by=['Yr', 'Team']).reset_index(drop=True)\n",
    "Series['Team'] = Series.Team.str.split(pat='(', expand=True)[0].str.rstrip()\n",
    "Series['Team.1'] = Series['Team.1'].str.split(pat='(', expand=True)[0].str.rstrip()\n",
    "Series['G'] = Series['W']+Series['W.1']\n",
    "Series = Series.rename(columns={\"Yr\": \"Year\", \"Team\": 'Tm', \"Team.1\":  \"Team\"})\n",
    "Series.Team[Series['Team']=='Washington Bullets'] = 'Washington Wizards'\n",
    "Series.Tm[Series['Tm']=='Washington Bullets'] = 'Washington Wizards'\n",
    "\n",
    "#Merging the opp. eFG% feature to the Series datast from the team dataset\n",
    "Series = pd.merge(Series,team[['Year', 'Team', 'Opp. EFg%']], on=['Year', 'Team'], how='left')\n",
    "\n",
    "#Calculating the weighted average of oppnent eFG%\n",
    "g = Series.groupby(['Year', 'Tm'])\n",
    "Series['WA_opp_FG%'] = Series.G / g.G.transform(\"sum\") * Series['Opp. EFg%']\n",
    "g = g[['WA_opp_FG%']].sum()\n",
    "g=g.reset_index()\n",
    "g = g.rename(columns={\"Tm\": 'Team'})\n",
    "Team_key = pd.read_csv('Team_key.csv')\n",
    "g = pd.merge(g, Team_key, on=['Team'], how='left')\n",
    "g = g.rename(columns={\"key\": 'Tm'})\n",
    "\n",
    "#Merge to the main dataset\n",
    "data = pd.merge(data, g[['Year', 'Tm', 'WA_opp_FG%']], on=['Year', 'Tm'], how='left')\n",
    "data = data.dropna(subset=['WA_opp_FG%'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I train the model I created the target variable, y, which indicates weather a player improved his eFG% in the playoffs compared with his regular season's eFG%. The dataframe X contains the relevant features of a player - including his personal stats and team stats.\n",
    "\n",
    "I have designed the code in a way that the target variable could be chosen and is assigned in the string variable 'target'. from there no changes needed for the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (4407, 49)\n"
     ]
    }
   ],
   "source": [
    "target = 'eFG%'\n",
    "data['target_diff']=data[target+'_Pfs']-data[target]\n",
    "data['target']=np.where(data['target_diff']>=0, 1, 0)\n",
    "rng = list(range(5,51))+list(range(54,57))\n",
    "X = data.iloc[:, rng]\n",
    "y = data['target'] \n",
    "\n",
    "print('X shape: ', X.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now, let split the dataset into training and test. After splitting, We can see that the dataset is not balanced when only 38% of the players in training set have improved their eFG% and only 35% in the test set. Therefore, predicting that all the players will not improve their eFG% will yield 65% accuracy on the test set.\n",
    "Since the data is not balanced, I chose to measure the model performance with roc-auc curve which would tell how much the model is able to distinguish between classes (identify 0s as 0s and 1s ans 1s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train mean:  0.38070921985815603\n",
      "y_test mean:  0.35034013605442177\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2\n",
    "                                ,random_state = 0)\n",
    "\n",
    "print('y_train mean: ', y_train.mean())\n",
    "print('y_test mean: ', y_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I train the model with a random forest classifier and use grid search in order to tune the parameters of the random forest classifier(such as max depth). I use 5-fold cross validation scheme and then test the best classifier on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (3525, 49)\n",
      "RF accuracy:  0.6621315192743764\n",
      "RF auc:  0.528236669547095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(random_state=0, n_estimators=100)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'n_estimators': [100, 200, 300], 'criterion': ['gini', 'entropy'], \n",
    "               'max_depth': [3,4,5,6]}]\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'roc_auc',\n",
    "                           cv=5,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators =best_parameters['n_estimators'],\n",
    "                                    criterion=best_parameters['criterion'],\n",
    "                                    max_depth=best_parameters['max_depth'],\n",
    "                                    random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('RF accuracy: ', (cm[1,1]+cm[0,0])/cm.sum())\n",
    "print('RF auc: ', roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the classifier barely learns anything form the dataset as the roc-auc score is ~53%. One of the reasons for that is that many players in the dataset didn't play enough and shoot the ball many times (regular season or in the playoffs). As we know form central limit theorem if the sample size is not large enough than sample average could be far from the true average and variance is larger.\n",
    "Hence, I decided to filter the dataset according to shots taken, minutes played and game played in order to keep only players that their averages in the regular season and playoffs are more stabilized. \n",
    "After applying the fiter, the roc-auc results improves to more than 58%, despite the fact that the model is trained on less observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (1467, 49)\n",
      "RF accuracy:  0.6839237057220708\n",
      "RF auc:  0.5827241557769439\n"
     ]
    }
   ],
   "source": [
    "data = data[(data['Total_FGA_Pfs']>50) & (data['Total_FGA']>50) ]\n",
    "data = data[(data['MP']>15) & (data['G']>20)].reset_index(drop=True)\n",
    "\n",
    "X = data.iloc[:, rng]\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2\n",
    "                                ,random_state = 0)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators =best_parameters['n_estimators'],\n",
    "                                    criterion=best_parameters['criterion'],\n",
    "                                    max_depth=best_parameters['max_depth'],\n",
    "                                    random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('RF accuracy: ', (cm[1,1]+cm[0,0])/cm.sum())\n",
    "print('RF auc: ', roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, I remove  highly correlated features (more than 0.9 correlation) as having both features doesn't contribute for prediction but adding some noise. Removing the features, improves the model roc_auc to 59%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrX = X.corr()\n",
    "columns = np.full((corrX.shape[0],), True, dtype=bool)\n",
    "for i in range(corrX.shape[0]):\n",
    "    for j in range(i+1, corrX.shape[0]):\n",
    "        if corrX.iloc[i,j] >= 0.9:\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "selected_columns = X.columns[columns]\n",
    "X = X[selected_columns]\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (1467, 35)\n",
      "RF accuracy:  0.6866485013623979\n",
      "RF auc:  0.58993724932074\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2\n",
    "                                ,random_state = 0)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators =best_parameters['n_estimators'],\n",
    "                                    criterion=best_parameters['criterion'],\n",
    "                                    max_depth=best_parameters['max_depth'],\n",
    "                                    random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('RF accuracy: ', (cm[1,1]+cm[0,0])/cm.sum())\n",
    "print('RF auc: ', roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, I check if there is more optimal combination of features using the RFECV (Recursive feature elimination cross validation) which remove features one by one and check if it improves the model according to the defined criteria. We can see that the 35 features left after removing the correlated features are the most optimal combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features: 35\n"
     ]
    }
   ],
   "source": [
    "rfecv = RFECV(estimator=classifier, step=1, cv=5, scoring='roc_auc',  n_jobs = -1)\n",
    "rfecv.fit(X_train, y_train)\n",
    "print('Optimal number of features: {}'.format(rfecv.n_features_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project I tried to predict which players improve their performance in the NBA playoffs compared to the NBA regular season. Using random forest algorithm and some feature engineering I achieved ~69% accuracy and 59% roc_auc score.\n",
    "Improvements to the model could be made as for the following:\n",
    "- add more features - especially game by game data as it might be we could get benefit from adding to the model features that describe the player performance against the specific teams he played in the playoffs.  \n",
    "- Train the model on more data -  my model was limited to data from 1997 and after which is the data available on the NBA Miner website."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
